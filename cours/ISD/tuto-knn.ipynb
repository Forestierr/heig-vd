{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7babef",
   "metadata": {},
   "source": [
    "# Tutorial Overview, kNN\n",
    "\n",
    "**Credit**: from “Machine Learning algorithms from scratch”, machinelearningmastery.com, Jason Brownlee\n",
    "https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/\n",
    "\n",
    "\n",
    "In this tutorial you are going to learn about the k-Nearest Neighbors algorithm including how it works and how to implement it from scratch in Python (without libraries).\n",
    "\n",
    "A simple but powerful approach for making predictions is to use the most similar historical examples to the new data. This is the principle behind the k-Nearest Neighbors algorithm.\n",
    "\n",
    "After completing this tutorial you will know:\n",
    "\n",
    "How to code the k-Nearest Neighbors algorithm step-by-step.\n",
    "How to evaluate k-Nearest Neighbors on a real dataset.\n",
    "How to use k-Nearest Neighbors to make a prediction for new data.\n",
    "Kick-start your project with my new book Machine Learning Algorithms From Scratch, including step-by-step tutorials and the Python source code files for all examples.\n",
    "\n",
    "This section will provide a brief background on the k-Nearest Neighbors algorithm that we will implement in this tutorial and the Abalone dataset to which we will apply it.\n",
    "\n",
    "### k-Nearest Neighbors\n",
    "\n",
    "The k-Nearest Neighbors algorithm or KNN for short is a very simple technique.\n",
    "\n",
    "The entire training dataset is stored. When a prediction is required, the k-most similar records to a new record from the training dataset are then located. From these neighbors, a summarized prediction is made.\n",
    "\n",
    "Similarity between records can be measured many different ways. A problem or data-specific method can be used. Generally, with tabular data, a good starting point is the Euclidean distance.\n",
    "\n",
    "Once the neighbors are discovered, the summary prediction can be made by returning the most common outcome or taking the average. As such, KNN can be used for classification or regression problems.\n",
    "\n",
    "There is no model to speak of other than holding the entire training dataset. Because no work is done until a prediction is required, KNN is often referred to as a lazy learning method.\n",
    "\n",
    "### Iris Flower Species Dataset\n",
    "\n",
    "In this tutorial we will use the Iris Flower Species Dataset.\n",
    "\n",
    "The Iris Flower Dataset involves predicting the flower species given measurements of iris flowers.\n",
    "\n",
    "It is a multiclass classification problem. The number of observations for each class is balanced. There are 150 observations with 4 input variables and 1 output variable. The variable names are as follows:\n",
    "\n",
    "Sepal length in cm.\n",
    "\n",
    "Sepal width in cm.\n",
    "\n",
    "Petal length in cm.\n",
    "\n",
    "Petal width in cm.\n",
    "\n",
    "Class\n",
    "\n",
    "The baseline performance on the problem is approximately 33%.\n",
    "Download the dataset and save it into your current working directory with the filename “iris.csv“.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c248f",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors (in 3 easy steps)\n",
    "\n",
    "First we will develop each piece of the algorithm in this section, then we will tie all of the elements together into a working implementation applied to a real dataset in the next section.\n",
    "\n",
    "This k-Nearest Neighbors tutorial is broken down into 3 parts:\n",
    "\n",
    "**Step 1**: Calculate Euclidean Distance.\n",
    "\n",
    "**Step 2**: Get Nearest Neighbors.\n",
    "\n",
    "**Step 3**: Make Predictions.\n",
    "\n",
    "These steps will teach you the fundamentals of implementing and applying the k-Nearest Neighbors algorithm for classification and regression predictive modeling problems.\n",
    "\n",
    "Note: This tutorial assumes that you are using Python 3.\n",
    "\n",
    "\n",
    "### Step 1: Calculate Euclidean Distance\n",
    "\n",
    "The first step is to calculate the distance between two rows in a dataset.\n",
    "\n",
    "Rows of data are mostly made up of numbers and an easy way to calculate the distance between two rows or vectors of numbers is to draw a straight line. This makes sense in 2D or 3D and scales nicely to higher dimensions.\n",
    "\n",
    "We can calculate the straight line distance between two vectors using the Euclidean distance measure. It is calculated as the square root of the sum of the squared differences between the two vectors.\n",
    "\n",
    "Euclidean Distance = sqrt(sum i to N (x1_i – x2_i)^2)\n",
    "\n",
    "Where x1 is the first row of data, x2 is the second row of data and i is the index to a specific column as we sum across all columns.\n",
    "\n",
    "With Euclidean distance, the smaller the value, the more similar two records will be. A value of 0 means that there is no difference between two records.\n",
    "\n",
    "Below is a function named **euclidean_distance()** that implements this in Python.\n",
    "\n",
    "You can see that the function assumes that the last column in each row is an output value which is ignored from the distance calculation.\n",
    "\n",
    "We can test this distance function with a small contrived classification dataset. We will use this dataset a few times as we construct the elements needed for the KNN algorithm.\n",
    "\n",
    "Putting this all together, we can write a small example to test our distance function by printing the distance between the first row and all other rows. We would expect the distance between the first row and itself to be 0, a good thing to look out for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37f6ec8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.3290173915275787\n",
      "1.9494646655653247\n",
      "1.5591439385540549\n",
      "0.5356280721938492\n",
      "4.850940186986411\n",
      "2.592833759950511\n",
      "4.214227042632867\n",
      "6.522409988228337\n",
      "4.985585382449795\n"
     ]
    }
   ],
   "source": [
    "# Example of calculating Euclidean distance\n",
    "from math import sqrt\n",
    " \n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    " \n",
    "# Test distance function\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "row0 = dataset[0]\n",
    "for row in dataset:\n",
    "\tdistance = euclidean_distance(row0, row)\n",
    "\tprint(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cedb78b",
   "metadata": {},
   "source": [
    "## Step 2: Get Nearest Neighbors\n",
    "\n",
    "Neighbors for a new piece of data in the dataset are the k closest instances, as defined by our distance measure.\n",
    "\n",
    "To locate the neighbors for a new piece of data within a dataset we must first calculate the distance between each record in the dataset to the new piece of data. We can do this using our distance function prepared above.\n",
    "\n",
    "Once distances are calculated, we must sort all of the records in the training dataset by their distance to the new data. We can then select the top k to return as the most similar neighbors.\n",
    "\n",
    "We can do this by keeping track of the distance for each record in the dataset as a tuple, sort the list of tuples by the distance (in descending order) and then retrieve the neighbors.\n",
    "\n",
    "Below is a function named **get_neighbors()** that implements this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb2093c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "\tdistances = list()\n",
    "\tfor train_row in train:\n",
    "\t\tdist = euclidean_distance(test_row, train_row)\n",
    "\t\tdistances.append((train_row, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\tneighbors = list()\n",
    "\tfor i in range(num_neighbors):\n",
    "\t\tneighbors.append(distances[i][0])\n",
    "\treturn neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60340cd4",
   "metadata": {},
   "source": [
    "You can see that the euclidean_distance() function developed in the previous step is used to calculate the distance between each train_row and the new test_row.\n",
    "\n",
    "The list of train_row and distance tuples is sorted where a custom key is used ensuring that the second item in the tuple (tup[1]) is used in the sorting operation.\n",
    "\n",
    "Finally, a list of the num_neighbors most similar neighbors to test_row is returned.\n",
    "\n",
    "We can test this function with the small contrived dataset prepared in the previous section.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f391156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.7810836, 2.550537003, 0]\n",
      "[3.06407232, 3.005305973, 0]\n",
      "[1.465489372, 2.362125076, 0]\n"
     ]
    }
   ],
   "source": [
    "# Example of getting neighbors for an instance\n",
    "from math import sqrt\n",
    " \n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    " \n",
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "\tdistances = list()\n",
    "\tfor train_row in train:\n",
    "\t\tdist = euclidean_distance(test_row, train_row)\n",
    "\t\tdistances.append((train_row, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\tneighbors = list()\n",
    "\tfor i in range(num_neighbors):\n",
    "\t\tneighbors.append(distances[i][0])\n",
    "\treturn neighbors\n",
    " \n",
    "# Test distance function\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "neighbors = get_neighbors(dataset, dataset[0], 3)\n",
    "for neighbor in neighbors:\n",
    "\tprint(neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c3475",
   "metadata": {},
   "source": [
    "Running this example prints the 3 most similar records in the dataset to the first record, in order of similarity.\n",
    "\n",
    "As expected, the first record is the most similar to itself and is at the top of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb970410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a classification prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "\tneighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "\toutput_values = [row[-1] for row in neighbors]\n",
    "\tprediction = max(set(output_values), key=output_values.count)\n",
    "\treturn prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706e55e",
   "metadata": {},
   "source": [
    "Now that we know how to get neighbors from the dataset, we can use them to make predictions.\n",
    "\n",
    "## Step 3: Make Predictions\n",
    "\n",
    "The most similar neighbors collected from the training dataset can be used to make predictions.\n",
    "\n",
    "In the case of classification, we can return the most represented class among the neighbors.\n",
    "\n",
    "We can achieve this by performing the **max()**  function on the list of output values from the neighbors. Given a list of class values observed in the neighbors, the max() function takes a set of unique class values and calls the count on the list of class values for each class value in the set.\n",
    "\n",
    "Below is the function **named predict_classification()** that implements this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dc55561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 0, Got 0.\n"
     ]
    }
   ],
   "source": [
    "# Example of making predictions\n",
    "from math import sqrt\n",
    " \n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    " \n",
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "\tdistances = list()\n",
    "\tfor train_row in train:\n",
    "\t\tdist = euclidean_distance(test_row, train_row)\n",
    "\t\tdistances.append((train_row, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\tneighbors = list()\n",
    "\tfor i in range(num_neighbors):\n",
    "\t\tneighbors.append(distances[i][0])\n",
    "\treturn neighbors\n",
    " \n",
    "# Make a classification prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "\tneighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "\toutput_values = [row[-1] for row in neighbors]\n",
    "\tprediction = max(set(output_values), key=output_values.count)\n",
    "\treturn prediction\n",
    " \n",
    "# Test distance function\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "prediction = predict_classification(dataset, dataset[0], 3)\n",
    "print('Expected %d, Got %d.' % (dataset[0][-1], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57f97bb",
   "metadata": {},
   "source": [
    "Running this example prints the expected classification of 0 and the actual classification predicted from the 3 most similar neighbors in the dataset.\n",
    "\n",
    "We can imagine how the predict_classification() function can be changed to calculate the mean value of the outcome values.\n",
    "\n",
    "We now have all of the pieces to make predictions with KNN. \n",
    "\n",
    "Let’s apply it to a real dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4da5622",
   "metadata": {},
   "source": [
    "## Iris Flower Species Case Study\n",
    "\n",
    "This section applies the KNN algorithm to the Iris flowers dataset.\n",
    "\n",
    "The first step is to load the dataset and convert the loaded data to numbers that we can use with the mean and standard deviation calculations. For this we will use the helper function **load_csv()** to load the file, str_column_to_float() to convert string numbers to floats and str_column_to_int() to convert the class column to integer values.\n",
    "\n",
    "We will evaluate the algorithm using k-fold cross-validation with 5 folds. This means that 150/5=30 records will be in each fold. We will use the helper functions evaluate_algorithm() to evaluate the algorithm with cross-validation and accuracy_metric() to calculate the accuracy of predictions.\n",
    "\n",
    "A new function named k_nearest_neighbors() was developed to manage the application of the KNN algorithm, first learning the statistics from a training dataset and using them to make predictions for a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f926f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [96.66666666666667, 96.66666666666667, 100.0, 90.0, 100.0]\n",
      "Mean Accuracy: 96.667%\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    " \n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "\tminmax = list()\n",
    "\tfor i in range(len(dataset[0])):\n",
    "\t\tcol_values = [row[i] for row in dataset]\n",
    "\t\tvalue_min = min(col_values)\n",
    "\t\tvalue_max = max(col_values)\n",
    "\t\tminmax.append([value_min, value_max])\n",
    "\treturn minmax\n",
    " \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)):\n",
    "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    " \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor _ in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    " \n",
    "# Calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    " \n",
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "\tdistances = list()\n",
    "\tfor train_row in train:\n",
    "\t\tdist = euclidean_distance(test_row, train_row)\n",
    "\t\tdistances.append((train_row, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\tneighbors = list()\n",
    "\tfor i in range(num_neighbors):\n",
    "\t\tneighbors.append(distances[i][0])\n",
    "\treturn neighbors\n",
    " \n",
    "# Make a prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "\tneighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "\toutput_values = [row[-1] for row in neighbors]\n",
    "\tprediction = max(set(output_values), key=output_values.count)\n",
    "\treturn prediction\n",
    " \n",
    "# kNN Algorithm\n",
    "def k_nearest_neighbors(train, test, num_neighbors):\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\toutput = predict_classification(train, row, num_neighbors)\n",
    "\t\tpredictions.append(output)\n",
    "\treturn(predictions)\n",
    " \n",
    "# Test the kNN on the Iris Flowers dataset\n",
    "seed(1)\n",
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "num_neighbors = 5\n",
    "scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6446434",
   "metadata": {},
   "source": [
    "Running the example prints the mean classification accuracy scores on each cross-validation fold as well as the mean accuracy score.\n",
    "\n",
    "We can see that the mean accuracy of about 96.6% is dramatically better than the baseline accuracy of 33%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c0141",
   "metadata": {},
   "source": [
    "We can use the training dataset to make predictions for new observations (rows of data).\n",
    "\n",
    "This involves making a call to the predict_classification() function with a row representing our new observation to predict the class label.\n",
    "\n",
    "Tying this together, a complete example of using KNN with the entire dataset and making a single prediction for a new observation is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cb9b91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iris-virginica] => 0\n",
      "[Iris-setosa] => 1\n",
      "[Iris-versicolor] => 2\n",
      "Data=[5.7, 2.9, 4.2, 1.3], Predicted: 2\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions with k-nearest neighbors on the Iris Flowers Dataset\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\t\tprint('[%s] => %d' % (value, i))\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    " \n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "\tminmax = list()\n",
    "\tfor i in range(len(dataset[0])):\n",
    "\t\tcol_values = [row[i] for row in dataset]\n",
    "\t\tvalue_min = min(col_values)\n",
    "\t\tvalue_max = max(col_values)\n",
    "\t\tminmax.append([value_min, value_max])\n",
    "\treturn minmax\n",
    " \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)):\n",
    "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    " \n",
    "# Calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    " \n",
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "\tdistances = list()\n",
    "\tfor train_row in train:\n",
    "\t\tdist = euclidean_distance(test_row, train_row)\n",
    "\t\tdistances.append((train_row, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\tneighbors = list()\n",
    "\tfor i in range(num_neighbors):\n",
    "\t\tneighbors.append(distances[i][0])\n",
    "\treturn neighbors\n",
    " \n",
    "# Make a prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "\tneighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "\toutput_values = [row[-1] for row in neighbors]\n",
    "\tprediction = max(set(output_values), key=output_values.count)\n",
    "\treturn prediction\n",
    " \n",
    "# Make a prediction with KNN on Iris Dataset\n",
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# define model parameter\n",
    "num_neighbors = 5\n",
    "# define a new record\n",
    "row = [5.7,2.9,4.2,1.3]\n",
    "# predict the label\n",
    "label = predict_classification(dataset, row, num_neighbors)\n",
    "print('Data=%s, Predicted: %s' % (row, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a317b1a4",
   "metadata": {},
   "source": [
    "Running the data first summarizes the mapping of class labels to integers and then fits the model on the entire dataset.\n",
    "\n",
    "Then a new observation is defined (in this case I took a row from the dataset), and a predicted label is calculated.\n",
    "\n",
    "In this case our observation is predicted as belonging to class 1 which we know is “Iris-setosa“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35da55d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
